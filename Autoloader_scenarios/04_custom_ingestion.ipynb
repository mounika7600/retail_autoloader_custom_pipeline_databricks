{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87eafee-52c1-48d0-b8c6-aae6b532ed2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    " %run ./00_common_imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47d22448-9ede-4409-8128-c403c557faea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01_autoloader_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb63bfee-30ad-4a42-aff8-ada91df6758c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# MAGIC %run ./00_common_imports\n",
    "# MAGIC %run ./01_autoloader_config\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "def custom_ingest(source_path, target_path, load_type=\"full\"):\n",
    "    \"\"\"\n",
    "    Custom ingestion logic using PySpark (manual CSV/JSON reader)\n",
    "    load_type = 'full' or 'incremental'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        log_info(f\"üöÄ Starting {load_type.upper()} load from {source_path} to {target_path}\")\n",
    "        \n",
    "        # 1Ô∏è‚É£ List all files in source\n",
    "        files = [f.path for f in dbutils.fs.ls(source_path) if f.path.lower().endswith(\".csv\")]\n",
    "        if not files:\n",
    "            log_warning(f\"No valid CSV files found in {source_path}\")\n",
    "            return\n",
    "        \n",
    "        for file in files:\n",
    "            log_info(f\"Reading file: {file}\")\n",
    "            df = (spark.read\n",
    "                  .option(\"header\", True)\n",
    "                  .option(\"inferSchema\", True)\n",
    "                  .csv(file)\n",
    "                  .withColumn(\"ingestion_ts\", current_timestamp()))\n",
    "            \n",
    "            # 2Ô∏è‚É£ Write to target\n",
    "            if load_type.lower() == \"full\":\n",
    "                df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n",
    "                log_info(f\"‚úÖ Full load completed successfully for {file}\")\n",
    "            else:\n",
    "                df.write.format(\"delta\").mode(\"append\").save(target_path)\n",
    "                log_info(f\"‚úÖ Incremental load completed successfully for {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log_error(f\"‚ùå Error during {load_type.upper()} load: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run Full Load\n",
    "custom_ingest(SOURCE_BASE, VOLUME_BASE, load_type=\"full\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# Run Incremental Load\n",
    "custom_ingest(SOURCE_BASE, VOLUME_BASE, load_type=\"incremental\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_custom_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
