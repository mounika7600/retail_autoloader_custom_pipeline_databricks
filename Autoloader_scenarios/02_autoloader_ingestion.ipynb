{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bda8e6-3fe4-4f06-84d0-985bfaad2192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01_autoloader_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b0aa21-583b-4a39-b8ee-40870a723af3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ingest_with_autoloader(src_path, tgt_path):\n",
    "    \"\"\"\n",
    "    Auto Loader ingestion with schema safety and corruption handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Detect format first\n",
    "        fmt = detect_file_format(src_path)\n",
    "        log(f\"Starting ingestion from {src_path}\")\n",
    "        log(f\"Detected format: {fmt}\")\n",
    "\n",
    "        if fmt is None:\n",
    "            log(f\"⚠️ Unsupported format for {src_path}, skipping.\")\n",
    "            return\n",
    "\n",
    "        # Define options common for all formats\n",
    "        options = {\n",
    "            \"cloudFiles.format\": fmt,\n",
    "            \"cloudFiles.inferColumnTypes\": \"true\",\n",
    "            \"cloudFiles.schemaEvolutionMode\": \"rescue\",\n",
    "            \"cloudFiles.maxFilesPerTrigger\": \"1\"\n",
    "        }\n",
    "\n",
    "        # Try reading one sample to test schema validity before streaming\n",
    "        try:\n",
    "            sample_df = spark.read.format(fmt).load(src_path)\n",
    "            # Normalize column names (remove special chars, spaces, tabs, etc.)\n",
    "            for c in sample_df.columns:\n",
    "                new_col = re.sub(r'[^a-zA-Z0-9_]', '_', c.strip())\n",
    "                sample_df = sample_df.withColumnRenamed(c, new_col)\n",
    "            log(f\"✅ Schema validated for {src_path}\")\n",
    "        except Exception as e:\n",
    "            log(f\"❌ Skipping {src_path} due to schema error: {e}\")\n",
    "            return  # Skip corrupted folder/file safely\n",
    "\n",
    "        # Proceed with Auto Loader\n",
    "        (\n",
    "            spark.readStream.format(\"cloudFiles\")\n",
    "            .options(**options)\n",
    "            .load(src_path)\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .option(\"checkpointLocation\", tgt_path + \"_checkpoint\")\n",
    "            .outputMode(\"append\")\n",
    "            .start(tgt_path)\n",
    "        )\n",
    "\n",
    "        log(f\"✅ Auto Loader started for {src_path} -> {tgt_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Failed ingestion for {src_path}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "02_autoloader_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
